# StoryWorld RunPod Serverless Worker
# RunPod Serverless. We set env vars in Console > Endpoint > Settings > Environment Variables.
# Base: runpod/base (if available) or nvidia/cuda

FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04

ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility
ENV HF_HOME=/worker/.cache/huggingface

RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    ffmpeg \
    libgl1 \
    libglib2.0-0 \
    libsm6 \
    libxext6 \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /worker

COPY requirements.gpu.txt .
RUN pip3 install --no-cache-dir -r requirements.gpu.txt runpod

# Pre-bake SDXL model (used by SVD backend when Gemini quota exhausted)
# Avoids runtime download that can be killed mid-transfer
RUN python3 -c "\
from diffusers import StableDiffusionXLPipeline; \
import torch; \
print('[build] Pre-downloading SDXL (stabilityai/stable-diffusion-xl-base-1.0)...'); \
StableDiffusionXLPipeline.from_pretrained('stabilityai/stable-diffusion-xl-base-1.0', torch_dtype=torch.float16); \
print('[build] SDXL cached successfully'); \
"

COPY agents ./agents
COPY models ./models
COPY worker.py ./
COPY worker_serverless.py ./

# RunPod Serverless expects handler
CMD ["python3", "-u", "worker_serverless.py"]
